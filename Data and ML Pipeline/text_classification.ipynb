{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_posts(input_lines, target_tag, split):\n",
    "    num = 1\n",
    "    train_csv_path = os.path.join(\"data\", \"prepared\", \"train.tsv\")\n",
    "    test_csv_path = os.path.join(\"data\", \"prepared\", \"test.tsv\")\n",
    "    os.makedirs(os.path.join(\"data\", \"prepared\"), exist_ok=True)\n",
    "\n",
    "    train = open(train_csv_path, \"w\", encoding=\"utf-8\")\n",
    "    test = open(test_csv_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    for line in input_lines:\n",
    "        try:\n",
    "            fd_out = train if random.random() > split else test\n",
    "            attr = xml.etree.ElementTree.fromstring(line).attrib\n",
    "\n",
    "            pid = attr.get(\"Id\", \"\")\n",
    "            label = 1 if target_tag in attr.get(\"Tags\", \"\") else 0\n",
    "            title = re.sub(r\"\\s+\", \" \", attr.get(\"Title\", \"\")).strip()\n",
    "            body = re.sub(r\"\\s+\", \" \", attr.get(\"Body\", \"\")).strip()\n",
    "            text = title + \" \" + body\n",
    "\n",
    "            fd_out.write(\"{}\\t{}\\t{}\\n\".format(pid, label, text))\n",
    "\n",
    "            num += 1\n",
    "        except Exception as ex:\n",
    "            sys.stderr.write(f\"Skipping the broken line {num}: {ex}\\n\")\n",
    "    train.close()\n",
    "    test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"data/data.xml\"\n",
    "input_lines = []\n",
    "with open(input) as fd_in:\n",
    "    input_lines = fd_in.readlines()\n",
    "split = 0.20\n",
    "seed = 47\n",
    "process_posts(input_lines=input_lines, target_tag=\"<r>\", split=split,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sparse\n",
    "import yaml\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(data):\n",
    "    \"\"\"Read the input data file and return a data frame.\"\"\"\n",
    "    df = pd.read_csv(data, encoding=\"utf-8\", header=None, delimiter=\"\\t\", names=[\"id\", \"label\", \"text\"])\n",
    "    sys.stderr.write(f\"The input data frame {data} size is {df.shape}\\n\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_train_features(bag_of_words, tfidf):\n",
    "    \"\"\"\n",
    "    Generate train feature matrix. \"\"\"\n",
    "    \n",
    "    df_train = get_df(\"data/prepared/train.tsv\")\n",
    "    train_words = np.array(df_train.text.str.lower().values)\n",
    "\n",
    "   \n",
    "    bag_of_words.fit(train_words)\n",
    "\n",
    "    train_words_binary_matrix = bag_of_words.transform(train_words)\n",
    "    feature_names = bag_of_words.get_feature_names_out()\n",
    "\n",
    "    tfidf.fit(train_words_binary_matrix)\n",
    "    train_words_tfidf_matrix = tfidf.transform(train_words_binary_matrix)\n",
    "    \n",
    "    out_path =\"data/features\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    train_output = os.path.join(\"data/features/train.pkl\")\n",
    "    save_matrix(df_train, train_words_tfidf_matrix, feature_names, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_test_features(bag_of_words, tfidf):\n",
    "    \"\"\"\n",
    "    Generate test feature matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_test = get_df(\"data/prepared/test.tsv\")\n",
    "    test_words = np.array(df_test.text.str.lower().values)\n",
    "\n",
    "    test_words_binary_matrix = bag_of_words.transform(test_words)\n",
    "    test_words_tfidf_matrix = tfidf.transform(test_words_binary_matrix)\n",
    "    feature_names = bag_of_words.get_feature_names_out()\n",
    "\n",
    "    out_path =\"data/features\"\n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    test_output = os.path.join(\"data/features/test.pkl\")\n",
    "    save_matrix(df_test, test_words_tfidf_matrix, feature_names, test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_matrix(df, matrix, names, output):\n",
    "    \"\"\"\n",
    "    Save the matrix to a pickle file.\n",
    "\n",
    "    \"\"\"\n",
    "    id_matrix = sparse.csr_matrix(df.id.astype(np.int64)).T\n",
    "    label_matrix = sparse.csr_matrix(df.label.astype(np.int64)).T\n",
    "\n",
    "    result = sparse.hstack([id_matrix, label_matrix, matrix], format=\"csr\")\n",
    "\n",
    "    msg = \"The output matrix {} size is {} and data type is {}\\n\"\n",
    "    sys.stderr.write(msg.format(output, result.shape, result.dtype))\n",
    "    \n",
    "    with open(output, \"wb\") as fd:\n",
    "        pickle.dump((result, names), fd)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "max_features = 100\n",
    "ngrams = 1\n",
    "\n",
    "bag_of_words = CountVectorizer(\n",
    "    stop_words=\"english\", max_features=max_features, ngram_range=(1, ngrams)\n",
    ")\n",
    "tfidf = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "generate_and_save_train_features(bag_of_words, tfidf)\n",
    "\n",
    "generate_and_save_test_features(bag_of_words, tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "import yaml\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Train a random forest classifier.\n",
    "    \n",
    "    Returns:\n",
    "        sklearn.ensemble.RandomForestClassifier: Trained classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    input = \"data/features\" \n",
    "\n",
    "    \n",
    "    seed = 47 \n",
    "    n_est = 100\n",
    "    min_split = 0.01\n",
    "    \n",
    "    # Load the data\n",
    "    with open(os.path.join(input, \"train.pkl\"), \"rb\") as fd:\n",
    "        matrix, _ = pickle.load(fd)\n",
    "    \n",
    "    labels = np.squeeze(matrix[:, 1].toarray())\n",
    "    x = matrix[:, 2:]\n",
    "\n",
    "    sys.stderr.write(\"Input matrix size {}\\n\".format(matrix.shape))\n",
    "    sys.stderr.write(\"X matrix size {}\\n\".format(x.shape))\n",
    "    sys.stderr.write(\"Y matrix size {}\\n\".format(labels.shape))\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_est, min_samples_split=min_split, n_jobs=2, random_state=seed\n",
    "    )\n",
    "\n",
    "    clf.fit(x, labels)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = train()\n",
    "\n",
    "# Save the model\n",
    "output = \"model.pkl\"\n",
    "with open(output, \"wb\") as fd:\n",
    "    pickle.dump(clf, fd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from dvclive import Live\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, matrix, split, save_path):\n",
    "    \"\"\"\n",
    "    Dump all evaluation metrics and plots for given datasets.\n",
    "\n",
    "    \"\"\"\n",
    "    labels = matrix[:, 1].toarray().astype(int)\n",
    "    x = matrix[:, 2:]\n",
    "    sys.stderr.write(\"Input matrix size {}\\n\".format(x.shape))\n",
    "    predictions_by_class = model.predict_proba(x)\n",
    "    predictions = predictions_by_class[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_importance_plot( model, feature_names):\n",
    "    \"\"\"\n",
    "    Save feature importance plot.\n",
    "\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(dpi=100)\n",
    "    fig.subplots_adjust(bottom=0.2, top=0.95)\n",
    "    axes.set_ylabel(\"Mean decrease in impurity\")\n",
    "\n",
    "    importances = model.feature_importances_\n",
    "    forest_importances = pd.Series(importances, index=feature_names).nlargest(n=30)\n",
    "    forest_importances.plot.bar(ax=axes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "EVAL_PATH = \"eval\"\n",
    "\n",
    "model_file = \"model.pkl\"\n",
    "feature_path = \"data/features\"\n",
    "\n",
    "# Load model and data.\n",
    "with open(model_file, \"rb\") as fd:\n",
    "    model = pickle.load(fd)\n",
    "\n",
    "with open(os.path.join(feature_path, \"train.pkl\"), \"rb\") as fd:\n",
    "    train, feature_names = pickle.load(fd)\n",
    "\n",
    "with open(os.path.join(feature_path, \"test.pkl\"), \"rb\") as fd:\n",
    "    test, feature_names = pickle.load(fd)\n",
    "\n",
    "# Evaluate train and test datasets.\n",
    "\n",
    "evaluate(model, train, \"train\", save_path=EVAL_PATH)\n",
    "evaluate(model, test, \"test\", save_path=EVAL_PATH)\n",
    "\n",
    "# Dump feature importance plot.\n",
    "save_importance_plot(model, feature_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
